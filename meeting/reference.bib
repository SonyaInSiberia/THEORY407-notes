@inproceedings{Chuan_Herremans,
  title   = {Modeling temporal tonal relations in polyphonic music through Deep Networks with a novel image-based representation},
  url     = {https://ojs.aaai.org/index.php/AAAI/article/view/11880},
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  author  = {Chuan, Ching-Hua and Herremans, Dorien}
} 
@inproceedings{springerChordalEmbeddings,
  author       = {},
  title        = {{C}hordal {E}mbeddings {B}ased on {T}opology of the {T}onal {S}pace --- link.springer.com},
  howpublished = {\url{https://link.springer.com/chapter/10.1007/978-3-031-29956-8_2}},
  year         = {},
  note         = {[Accessed 25-09-2023]}
}

@misc{MusBERT,
  author       = {},
  title        = {{M}usic{B}{E}{R}{T}: {S}ymbolic {M}usic {U}nderstanding with {L}arge-{S}cale {P}re-{T}raining --- arxiv.org},
  howpublished = {\url{https://arxiv.org/abs/2106.05630}},
  year         = {},
  note         = {[Accessed 25-09-2023]}
}

@article{survey,
  author     = {Ji, Shulei and Yang, Xinyu and Luo, Jing},
  title      = {A Survey on Deep Learning for Symbolic Music Generation: Representations, Algorithms, Evaluations, and Challenges},
  year       = {2023},
  issue_date = {January 2024},
  publisher  = {Association for Computing Machinery},
  address    = {New York, NY, USA},
  volume     = {56},
  number     = {1},
  issn       = {0360-0300},
  url        = {https://doi.org/10.1145/3597493},
  doi        = {10.1145/3597493},
  abstract   = {Significant progress has been made in symbolic music generation with the help of deep learning techniques. However, the tasks covered by symbolic music generation have not been well summarized, and the evolution of generative models for the specific music generation task has not been illustrated systematically. This paper attempts to provide a task-oriented survey of symbolic music generation based on deep learning techniques, covering most of the currently popular music generation tasks. The distinct models under the same task are set forth briefly and strung according to their motivations, basically in chronological order. Moreover, we summarize the common datasets suitable for various tasks, discuss the music representations and the evaluation methods, highlight current challenges in symbolic music generation, and finally point out potential future research directions.},
  journal    = {ACM Comput. Surv.},
  month      = {aug},
  articleno  = {7},
  numpages   = {39},
  keywords   = {deep learning, music evaluation methods, Symbolic music generation, task-oriented survey, symbolic music representations}
}

@inproceedings{pianoroll,
  author = {Angioloni, Luca and Borghuis, Tijn and Brusci, Lorenzo and Frasconi, Paolo},
  year   = {2020},
  month  = {10},
  pages  = {},
  title  = {Conlon: A pseudo-song generator based on a new pianoroll, wasserstein autoencoders, and optimal interpolations.}
}

@inproceedings{AAAI_tonnetz,
  author    = {Chuan, Ching-Hua and Herremans, Dorien},
  title     = {Modeling Temporal Tonal Relations in Polyphonic Music through Deep Networks with a Novel Image-Based Representation},
  year      = {2018},
  isbn      = {978-1-57735-800-8},
  publisher = {AAAI Press},
  abstract  = {We propose an end-to-end approach for modeling polyphonic music with a novel graphical representation, based on music theory, in a deep neural network. Despite the success of deep learning in various applications, it remains a challenge to incorporate existing domain knowledge in a network without affecting its training routines. In this paper we present a novel approach for predictive music modeling and music generation that incorporates domain knowledge in its representation. In this work, music is transformed into a 2D representation, inspired by tonnetz from music theory, which graphically encodes musical relationships between pitches. This representation is incorporated in a deep network structure consisting of multilayered convolutional neural networks (CNN, for learning an efficient abstract encoding of the representation) and recurrent neural networks with long short-term memory cells (LSTM, for capturing temporal dependencies in music sequences). We empirically evaluate the nature and the effectiveness of the network by using a dataset of classical music from various composers. We investigate the effect of parameters including the number of convolution feature maps, pooling strategies, and three configurations of the network: LSTM without CNN, LSTM with CNN (pre-trained vs. not pre-trained). Visualizations of the feature maps and filters in the CNN are explored, and a comparison is made between the proposed tonnetz-inspired representation and pianoroll, a commonly used representation of music in computational systems. Experimental results show that the tonnetz representation produces musical sequences that are more tonally stable and contain more repeated patterns than sequences generated by pianoroll-based models, a finding that is directly useful for tackling current challenges in music and AI such as smart music generation.},
  booktitle = {Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence and Thirtieth Innovative Applications of Artificial Intelligence Conference and Eighth AAAI Symposium on Educational Advances in Artificial Intelligence},
  articleno = {263},
  numpages  = {8},
  location  = {New Orleans, Louisiana, USA},
  series    = {AAAI'18/IAAI'18/EAAI'18}
}
@article{yang,
  author     = {Yang, Li-Chia and Lerch, Alexander},
  title      = {On the Evaluation of Generative Models in Music},
  year       = {2020},
  issue_date = {May 2020},
  publisher  = {Springer-Verlag},
  address    = {Berlin, Heidelberg},
  volume     = {32},
  number     = {9},
  issn       = {0941-0643},
  url        = {https://doi.org/10.1007/s00521-018-3849-7},
  doi        = {10.1007/s00521-018-3849-7},
  abstract   = {The modeling of artificial, human-level creativity is becoming more and more achievable. In recent years, neural networks have been successfully applied to different tasks such as image and music generation, demonstrating their great potential in realizing computational creativity. The fuzzy definition of creativity combined with varying goals of the evaluated generative systems, however, makes subjective evaluation seem to be the only viable methodology of choice. We review the evaluation of generative music systems and discuss the inherent challenges of their evaluation. Although subjective evaluation should always be the ultimate choice for the evaluation of creative results, researchers unfamiliar with rigorous subjective experiment design and without the necessary resources for the execution of a large-scale experiment face challenges in terms of reliability, validity, and replicability of the results. In numerous studies, this leads to the report of insignificant and possibly irrelevant results and the lack of comparability with similar and previous generative systems. Therefore, we propose a set of simple musically informed objective metrics enabling an objective and reproducible way of evaluating and comparing the output of music generative systems. We demonstrate the usefulness of the proposed metrics with several experiments on real-world data.},
  journal    = {Neural Comput. Appl.},
  month      = {may},
  pages      = {4773–4784},
  numpages   = {12},
  keywords   = {Computational creativity, Music generation, Objective evaluation}
}